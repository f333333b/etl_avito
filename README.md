# ETL-пайплайн для работы с объявлениями Авито через автозагрузку
![Airflow](https://img.shields.io/badge/Airflow-2.8+-brightgreen)
![Python](https://img.shields.io/badge/Python-3.10+-blue)
![Pandas](https://img.shields.io/badge/pandas-2.0+-150458?logo=pandas&logoColor=white&labelColor=black)
![Docker](https://img.shields.io/badge/docker-ready-blue?logo=docker&logoColor=white)
![License](https://img.shields.io/github/license/f333333b/etl_avito)
![GitHub last commit](https://img.shields.io/github/last-commit/f333333b/etl_avito)


## Описание проекта

Проект реализует полноценный ETL-процесс (`Extract` → `Transform` → `Validate` → `Load`) для обработки данных автозагрузки объявлений на платформе Авито.
Входные данные: по умолчанию обрабатывается тестовый файл `.xlsx` Также поддерживаются `.xls`, `.csv`.
Используется в ежедневной работе компании, занимающейся продажей техники на Авито.
Объем: обработка около 20 файлов, каждый объёмом ~50 000 строк.

---

### Функциональность:

- Автоматическая очистка и нормализация данных.
- Проверка уникальности и целостности.
- Проверка соответствия требованиям тех. документации Авито с помощью справочника.
- Валидация дилерских ограничений по городам и брендам.
- Проверка и дублирование объявлений для размещения каждой единицы техники во всех филиалах дилера (если требуется).
- Логирование ключевых проблем для ручного анализа.
- Вывод аналитической информации для корректировки бизнес-стратегии.

---

### Цели, которые решает проект:

- Упростить, ускорить и стандартизировать процесс обработки данных.
- Повысить точность и эффективность размещения объявлений.
- Предоставить бизнесу данные для принятия решений по построению стратегии продаж.

---

## Стек

- Python 3.10+
- pandas
- openpyxl
- Docker, Docker Compose
- Airflow
- requests
- aiohttp

---

<details>
<summary><strong>Структура проекта (нажать для открытия)</strong></summary>

```bash
etl_avito/
│
├── dags/                         
│   └── etl_avito_dag.py          # DAG для Airflow: описание этапов ETL-процесса и порядок их выполнения
│
├── etl/                          
│   ├── __init__.py               # делает директорию etl Python-пакетом
│   ├── config.py                 # загрузка конфигураций, переменных окружения
│   ├── extract.py                # логика извлечения данных из исходных файлов
│   ├── load.py                   # сохранение обработанных данных в файл или БД
│   ├── pipeline_config.yaml      # YAML-конфигурация пайплайна (список трансформаций и настройки)
│   ├── transform.py              # функции очистки, нормализации и трансформации данных
│   ├── utils.py                  # вспомогательные утилиты и функции
│   ├── validation.py             # валидация данных (проверки целостности, формата и справочников)
│   └── data/                     
│       ├── __init__.py           # делает директорию data Python-пакетом
│       ├── avito_data.parquet    # промежуточный parquet-файл с данными после извлечения или трансформации
│       ├── input_data.md         # описание или пример структуры исходных данных
│       ├── input_sample.xlsx     # тестовый Excel-файл с объявлениями (используется для проверки пайплайна)
│       └── reference_data.py     # справочники городов, брендов и других констант
│
├── .env                          # основной файл переменных окружения (не хранится в репозитории)
├── .env.example                  # пример .env с переменными окружения для локальной разработки
├── .flake8                       # конфигурация линтера Flake8
├── .gitignore                    # список файлов и папок, игнорируемых Git
├── .pre-commit-config.yaml       # конфигурация хуков pre-commit
├── docker-compose.yaml           # описание сервисов для запуска проекта через Docker Compose
├── Dockerfile                    # сборка собственного образа Airflow для проекта
├── mypy.ini                      # настройки статической типизации (mypy)
├── pyproject.toml                # единый конфигурационный файл для сборщиков и линтеров (например, Black, isort)
├── README.md                     # описание проекта, установка, запуск и документация
├── requirements.txt              # список зависимостей для работы проекта
└── requirements-dev.txt          # список зависимостей для разработки (тесты, линтеры, mypy и др.)

```
</details> 


## Этапы обработки

### 1. Extract

Чтение Excel-файла с объявлениями (.xlsx).

### 2. Transform

- удаление мусорных/пустых строк
- проверка на уникальность значений столбцов AvitoId и Id
- нормализация строк по столбцу Title
- фильтрация и корректировка значений Address в соответствии со справочником городов
- удаление строк, нарушающих дилерство
- обеспечение полного размещения техники по разрешённым адресам

Все действия логируются. Проблемные записи (дубликаты, неизвестные адреса, нарушения) сохраняются отдельно или выводятся в лог.

### 3. Load

Создание на основе обработанного DataFrame реляционной базы данных для дальнейших действий:

- создания дашбордов
- аналитики
- выполнение SQL-запросов

## Логика дилерства

Справочник `dealerships` указывает, в каких городах разрешено размещение объявлений для каждого бренда. Все нарушения автоматически отсеиваются и логируются по AvitoID.

---

## Установка

### Установка зависимостей

Создайте виртуальное окружение и активируйте его:

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Настройка переменных окружения

Создайте файл `.env` на основе шаблона `.env.example`:

```bash
cp .env.example .env
```

Переменные также можно задать через Airflow Variables (раздел Admin → Variables в веб-интерфейсе Airflow).

### Входные данные

- По умолчанию обрабатывается тестовый файл:

```
etl_avito/data/sample.xlsx
```

- Чтобы использовать свой файл, задайте переменную окружения `INPUT_PATH`:

    - либо в `.env`
    - либо в Airflow Variables → INPUT_PATH

Файл должен быть расположен в папке:

```
etl_avito/data/
```

### Запуск проекта

Убедитесь, что скопирован файл `.env` и установлены все зависимости Docker или Docker Compose.

Сначала соберите docker-образы (если не собраны ранее):

```bash
docker-compose build
```

Запустите Airflow:

```bash
docker-compose up -d
```

Airflow UI доступен по адресу:

[http://localhost:8080](http://localhost:8080)


Логин по умолчанию:

- Username: admin
- Password: admin

Запустите DAG `etl_avito_dag`

## Использование в Airflow

- DAG называется `etl_avito_dag`.
- DAG можно запустить вручную через UI или командой:

```bash
docker-compose run webserver airflow dags trigger etl_avito_dag
```

## TODO

### LOAD

- запуск автозагрузки файла через Avito API (по необходимости через параметр CLI или флаг)
- создать на основе обработанного DataFrame реляционную базу данных в 3NF
- создать `schema.sql` — документацию по структуре БД

### BI (дашборд)

- сводная информация об изменениях по результатам ETL
- количество всех объявлений
- количество объявлений по каждой категории (мото, квадро и т.д.) — график
- количество объявлений по каждому адресу — график
- количество объявлений с прикреплённым видео (столбец VideoURL)
- количество объявлений с коротким видео (столбец VideoFilesURL)

### Utility

- Unit-тесты
- CI/CD
- HTML-отчёт о результатах работы DAG, отправка по почте и в Telegram
- Метрики производительности обработки нескольких файлов по ~50 000 строк
- Писать логи в файлы
