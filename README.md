# ETL-пайплайн для работы с Avito-объявлениями через автозагрузку

## Описание проекта

Данный проект реализует полноценный ETL-процесс (Extract → Transform → Load) для обработки данных автозагрузки объявлений на платформе Avito.

Используется в ежедневной работе компании, занимающейся продажей техники на Avito. Обрабатывает объёмы более 100 000 объявлений.

---

### Функциональность пайплайна:

- Автоматическая очистка и нормализация данных.
- Проверка уникальности и целостности.
- Валидация дилерских ограничений по городам и брендам.
- Логирование ключевых проблем для ручного анализа.
- Вывод аналитической информации для корректировки бизнес-стратегии.

---

### Цели, которые решает проект:

- Упростить, ускорить и стандартизировать процесс обработки данных.
- Повысить точность и эффективность размещения объявлений.
- Предоставить бизнесу данные для принятия решений по построению стратегии продаж.

---

## Стек

- Python 3.10+
- pandas
- openpyxl
- Docker, Docker Compose
- Airflow

---

## Структура проекта

```bash
etl_avito/
│
├── data/                      # входные и выходные файлы (sample.xlsx, parquet, логи)
├── extract.py                 # извлечение данных
├── transform.py               # очистка, нормализация, валидация, фильтрация
├── load.py                    # сохранение результатов
├── reference_data.py          # справочник городов и брендов
├── requirements.txt           # зависимости
├── README.md                  # описание проекта
├── dags/                      # DAG для Airflow
└── scripts/                   # отдельные скрипты

Этапы обработки
1. Extract

Чтение Excel-файла с объявлениями (.xlsx).
2. Transform

    удаление мусорных/пустых строк

    проверка на уникальность значений столбцов AvitoId и Id

    нормализация строк по столбцу Title

    фильтрация и корректировка значений Address в соответствии со справочником городов

    удаление строк, нарушающих дилерство

    обеспечение полного размещения техники по разрешённым адресам

Все действия логируются. Проблемные записи (дубликаты, неизвестные адреса, нарушения) сохраняются отдельно или выводятся в лог.
3. Load

Создание на основе обработанного DataFrame реляционной базы данных для дальнейших действий:

    создания дашбордов

    аналитики

    выполнение SQL-запросов

Логика дилерства

Справочник dealerships указывает, в каких городах разрешено размещение объявлений для каждого бренда. Все нарушения автоматически отсеиваются и логируются по AvitoID.
Установка
Установка зависимостей

Создайте виртуальное окружение и активируйте его:

python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

Настройка переменных окружения

Создайте файл .env на основе шаблона .env.example:

cp .env.example .env

Переменные также можно задать через Airflow Variables
(раздел Admin → Variables в веб-интерфейсе Airflow).
Входные данные

    По умолчанию обрабатывается тестовый файл:

etl_avito/data/sample.xlsx

    Чтобы использовать свой файл, задайте переменную окружения INPUT_PATH:

        либо в .env

        либо в Airflow Variables → INPUT_PATH

Файл должен быть расположен в папке:

etl_avito/data/

Запуск проекта

Убедитесь, что скопирован файл .env и установлены все зависимости Docker или Docker Compose.

Запустите Airflow:

docker-compose up -d

Airflow UI доступен по адресу:

http://localhost:8080

Логин по умолчанию:

    Username: admin

    Password: admin

Запустите DAG etl_avito_dag вручную или дождитесь следующего расписания.
Использование в Airflow

    DAG называется etl_avito_dag.

    DAG можно запустить вручную через UI или командой:

docker-compose run webserver airflow dags trigger etl_avito_dag

    Чтобы указать путь к своему входному файлу, измените переменную INPUT_PATH в Airflow Variables или в файле .env.

TODO
LOAD

    запуск автозагрузки файла через Avito API (по необходимости через параметр CLI или флаг)

    создать на основе обработанного DataFrame реляционную базу данных в 3NF

    создать schema.sql — документацию по структуре БД

BI (дашборд)

    сводная информация об изменениях по результатам ETL

    количество всех объявлений

    количество объявлений по каждой категории (мото, квадро и т.д.) — график

    количество объявлений по каждому адресу — график

    количество объявлений с прикреплённым видео (столбец VideoURL)

    количество объявлений с коротким видео (столбец VideoFilesURL)

Utility

    Unit-тесты

    Docker

    CI/CD

    Airflow

    Уведомления в Telegram о результатах работы DAG

    Метрики производительности обработки 100 000 записей

    Писать логи в файлы

    HTML-отчёт, пригодный для отправки по почте или в Telegram

Полезные заметки

    При запуске Docker требуются sudo-права.

    При первой сборке Docker создайте файл .env на основе .env.example.

    Airflow доступен по адресу http://localhost:8080.

    Доступ по умолчанию в Airflow: admin/admin.

    По умолчанию обрабатывается тестовый файл sample.xlsx.

    Для проверки работы с реальными данными используйте Airflow Variables → INPUT_PATH.